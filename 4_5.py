# -*- coding: utf-8 -*-
"""4_5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HuZ6_UmFLs36uYwCPovr3pH9bqnR-0uk
"""

# Required libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

"""### Step 1: Load and Parse the Data

In this step, we load the retail sales data (CSV) and macroeconomic indicators (Excel), clean column names, and convert the date fields to datetime format. This ensures the datasets are aligned and ready for time-based analysis.


"""

import pandas as pd

# Load retail sales data (CSV)
df_retail = pd.read_csv('/content/retail_salse_long 2024.csv')
df_retail.columns = df_retail.columns.str.strip()  # clean column names
df_retail['month'] = pd.to_datetime(df_retail['month'], errors='coerce')

# Load macroeconomic data (Excel)
df_macro = pd.read_excel('/content/monthly_macro_data.xlsx')
df_macro.columns = df_macro.columns.str.strip()
df_macro['Date'] = pd.to_datetime(df_macro['Date'], errors='coerce')

# Show structure and sample for retail data
print("ðŸ›ï¸ Retail Sales Data Preview:")
print(df_retail.head())
print(f"ðŸ“… Date Range: {df_retail['month'].min().date()} to {df_retail['month'].max().date()}")
print(f"ðŸ§® Unique categories: {df_retail['kind_of_business'].nunique()}")

# Show structure and sample for macro data
print("\nðŸ“ˆ Macro Data Preview:")
print(df_macro.head())
print(f"ðŸ“… Date Range: {df_macro['Date'].min().date()} to {df_macro['Date'].max().date()}")

"""### Step 2: Handle Missing Values

We identify and remove missing values from the datasets, especially in the `value` column of the retail data, which is our key target variable. Zeros are kept, as they may represent true no-sales events. This cleaning ensures model inputs are valid.

"""

print("\nðŸ” Missing Values in Retail Data:")
print(df_retail.isnull().sum())

print("\nðŸ” Missing Values in Macro Data:")
print(df_macro.isnull().sum())


# Drop rows where sales value is missing
df_retail = df_retail.dropna(subset=['value'])

# Confirm it's cleaned
print("âœ… Missing 'value' entries after cleaning:", df_retail['value'].isnull().sum())

"""**Clean the Missing value Entries**

###  Step 3: Retail Sales Data Exploration

In this step:

Computed summary statistics for the value column representing raw sales

Identified the top 10 most frequent business categories by record count

Counted the total number of unique business types (65 in total)

Calculated descriptive statistics (mean, standard deviation, min, max, and count) for each business category

Flagged the least frequent categories, which may affect the stability and generalizability of our models

This step offers a foundational understanding of the data distribution across retail sectors, helping us evaluate balance and representativeness before proceeding with time series modeling and clustering.
"""

# ðŸ“‰ Step 3: Retail Sales Summary After Cleaning

# Overall summary statistics of cleaned retail sales values
print("\nOverall Retail Sales Summary Statistics (After Cleaning):")
print(df_retail['value'].describe())

# Frequency of each business category
category_counts = df_retail['kind_of_business'].value_counts()
print("\nTop 10 Business Categories by Frequency:")
print(category_counts.head(10))

# Total number of unique business categories
num_categories = df_retail['kind_of_business'].nunique()
print(f"\nTotal Number of Business Categories: {num_categories}")

# Summary statistics (mean, std, min, max, count) for each business category
category_stats = df_retail.groupby('kind_of_business')['value'].describe()
print("\nPer-Category Sales Summary Statistics:")
print(category_stats.head())  # Preview first 5 categories

# Least frequent category or categories
min_count = category_counts.min()
least_common_categories = category_counts[category_counts == min_count]
print(f"\nMinimum Number of Records in Any Category: {min_count}")
print("Categories with the Least Records:")
print(least_common_categories)

# Re-align and merge trimmed datasets
df_retail_trimmed = df_retail[
    (df_retail['month'] >= df_macro['Date'].min()) &
    (df_retail['month'] <= df_macro['Date'].max())
].copy()
df_macro_trimmed = df_macro[
    (df_macro['Date'] >= df_retail['month'].min()) &
    (df_macro['Date'] <= df_retail['month'].max())
].copy()
df_retail_trimmed = df_retail_trimmed.dropna(subset=['value']).copy()
df_retail_trimmed = df_retail_trimmed.rename(columns={'month': 'Date'})
df_merged = pd.merge(df_retail_trimmed, df_macro_trimmed, on='Date', how='inner')
df_merged = df_merged.drop(columns=['naics_code'])

# Step: Create sales growth (log difference of raw sales)
df_merged['sales_growth'] = df_merged.groupby('kind_of_business')['value'].transform(
    lambda x: np.log(x) - np.log(x.shift(1))
)

# ðŸ“Š Summary statistics for inflation, GDP growth, raw sales, and sales growth
print("\nSummary Statistics for Inflation, GDP Growth, Raw Sales, and Sales Growth:")

# Compute and format summary statistics
summary_stats = df_merged[['Inflation', 'gdp_growth_m', 'value', 'sales_growth']].describe().T
summary_stats = summary_stats[['mean', 'std', 'min', 'max']].round(4)
summary_stats.index = [
    'Inflation (CPI log-diff)',
    'GDP Growth (monthly interp.)',
    'Retail Sales (raw value)',
    'Sales Growth (log-diff)',
]

# Display table
print(summary_stats)

"""###  Step 4: Align Retail and Macro Data by Shared Date Range

In this step, we:
- Identify the overlapping time window between the retail and macroeconomic datasets
- Trim both datasets to this shared range
- Re-check and clean any missing `value` entries that may remain after trimming

This ensures consistency for time-series analysis and safe merging in the next step.

"""

# ðŸ“† Step 4: Align Retail and Macro Data to Shared Date Range (after cleaning)

# Step 1: Get min and max dates for each dataset
retail_start = df_retail['month'].min()
retail_end = df_retail['month'].max()
macro_start = df_macro['Date'].min()
macro_end = df_macro['Date'].max()

# Step 2: Compute the overlapping shared date range
shared_start = max(retail_start, macro_start)
shared_end = min(retail_end, macro_end)
print(f"âœ… Shared date range: {shared_start.date()} to {shared_end.date()}")

# Step 3: Trim both datasets to the shared range
df_retail_trimmed = df_retail[(df_retail['month'] >= shared_start) & (df_retail['month'] <= shared_end)].copy()
df_macro_trimmed = df_macro[(df_macro['Date'] >= shared_start) & (df_macro['Date'] <= shared_end)].copy()

# Optional: Re-check for any remaining missing 'value' entries
df_retail_trimmed = df_retail_trimmed.dropna(subset=['value']).copy()

# Step 4: Confirm final date range
print(f"\nðŸ›ï¸ Retail data trimmed to: {df_retail_trimmed['month'].min().date()} â†’ {df_retail_trimmed['month'].max().date()}")
print(f"ðŸ“ˆ Macro data trimmed to: {df_macro_trimmed['Date'].min().date()} â†’ {df_macro_trimmed['Date'].max().date()}")

# Step 5: Merge Retail and Macro Datasets on Date (Moved from Step 5)
# Make sure both have the same date column name
df_retail_trimmed = df_retail_trimmed.rename(columns={'month': 'Date'})

# Merge on 'Date'
df_merged = pd.merge(df_retail_trimmed, df_macro_trimmed, on='Date', how='inner')


# Drop 'naics_code' from the merged dataset
df_merged = df_merged.drop(columns=['naics_code'])

# Confirm it's removed
print("âœ… Columns after dropping 'naics_code':")
print(df_merged.columns)

"""### Step 5: Merge Retail and Macro Datasets

In this step, we merge the cleaned and date-aligned retail and macroeconomic datasets using the `Date` column as a key. This combined dataset allows us to:

- Model retail sales in relation to macroeconomic conditions (e.g., inflation, GDP)
- Detect structural change-points and elasticities
- Perform regression and clustering using both retail and macro features

"""

# Step 5: Merge Retail and Macro Datasets on Date (Content moved to Step 4)

# Preview merged dataset
print("\nâœ… Merged Dataset Preview:")
print(df_merged.head())

# Check final shape and missing values
print(f"\nðŸ§¾ Final merged shape: {df_merged.shape}")
print("\nâ“ Missing values in merged data:")
print(df_merged.isnull().sum())

# Add: Print date range of merged data
print(f"\nðŸ“… Date range in merged data: {df_merged['Date'].min().date()} to {df_merged['Date'].max().date()}")

"""###  Step 6: Feature Engineering


We generate new features to capture trends and structure in the sales data:

sales_growth: Month-over-month log change in sales, providing a stable measure of growth

sales_lag1: Previous monthâ€™s sales value

sales_rolling_mean: 3-month rolling average of sales

These features help uncover temporal dependencies and prepare the data for regression, clustering, and Markov switching analysis.

"""

# Step 6: Feature Engineering per Business Category (No Z-score)

# Ensure sorted by category and date
df_merged = df_merged.sort_values(['kind_of_business', 'Date'])

# 1ï¸âƒ£ Log Growth in Sales (safer than pct_change for low/zero values)
df_merged['value'] = df_merged['value'].replace(0, np.nan)  # avoid log(0)
df_merged['sales_growth'] = df_merged.groupby('kind_of_business')['value'].transform(
    lambda x: np.log(x).diff()
)

# 2ï¸âƒ£ Lagged Sales Value (1 month before)
df_merged['sales_lag1'] = df_merged.groupby('kind_of_business')['value'].shift(1)

# 3ï¸âƒ£ 3-Month Rolling Average of Sales
df_merged['sales_rolling_mean'] = df_merged.groupby('kind_of_business')['value'].transform(
    lambda x: x.rolling(window=3).mean()
)

# Preview the new features
print("\nâœ… Feature Engineering Complete (Growth-based). Sample output:")
print(df_merged.head(10))

"""### Create Cluster Summary for Correlation Analysis

Before plotting the correlation heatmap, we need to create a summary DataFrame (`df_summary`) that aggregates key statistics for each cluster. This involves:

1.  Merging the cluster labels (`df_cluster_labels`) with the main merged data (`df_merged`).
2.  Grouping the merged data by `cluster` and `kind_of_business`.
3.  Calculating summary statistics (like mean) for relevant numerical features within each business category.
4.  Aggregating these business-level statistics to the cluster level (e.g., averaging the means of businesses within a cluster).
5.  Counting the number of unique business series within each cluster.

This `df_summary` will then contain the data needed to compute the correlation matrix between cluster-level characteristics and macroeconomic indicators.
"""



"""### Step 6.1: Remove NaN Rows for Modeling

Now that all engineered features have been created, we remove rows with missing values. These NaNs are caused by lag, growth, and rolling operations at the beginning of each time series, and are safe to drop.

"""

# Drop rows with NaNs in any of the key engineered features
df_model = df_merged.dropna(subset=[
    'sales_growth',
    'sales_lag1',
    'sales_rolling_mean',
    'gdp_growth_m'
])

# Confirm
print(f"âœ… Cleaned dataset shape: {df_model.shape}")
print(f"ðŸ§¹ Any remaining NaNs? \n{df_model.isnull().sum()}")

!pip install tslearn

"""**DTW-Based Clustering of Retail Sales Patterns**
This code groups retail business categories into clusters by applying Dynamic Time Warping (DTW) on their normalized monthly sales trajectories, capturing similarities in their temporal evolution.
"""

from tslearn.clustering import TimeSeriesKMeans
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# Set a range of k to try
k_range = range(2, 10)
inertia = []

# Step 0: Calculate monthly log growth rate per business (Needed to define data_growth)
# Ensure no zero values before taking logs
df_merged['value'] = df_merged['value'].replace(0, np.nan)
df_merged['sales_growth'] = df_merged.groupby('kind_of_business')['value'].transform(lambda x: np.log(x).diff())

# Step 1: Drop rows with missing growth rates (Needed to define data_growth)
df_growth_cleaned = df_merged.dropna(subset=['sales_growth']).copy()

# Step 2: Pivot table to shape (Date Ã— Business) (Needed to define data_growth)
df_growth = df_growth_cleaned.groupby(['Date', 'kind_of_business'])['sales_growth'].mean().reset_index()
df_growth_matrix = df_growth.pivot(index='Date', columns='kind_of_business', values='sales_growth')

# Step 3: Transpose to (Businesses Ã— Time) (Needed to define data_growth)
data_growth = df_growth_matrix.T.fillna(0).values  # shape: (n_businesses, n_timepoints)


# Run clustering for each k
for k in k_range:
    model = TimeSeriesKMeans(n_clusters=k, metric="dtw", random_state=42)
    model.fit(data_growth)
    inertia.append(model.inertia_)  # total within-cluster DTW distance

# Plot the elbow
plt.figure(figsize=(8, 4))
plt.plot(k_range, inertia, marker='o')
plt.title("Elbow Method â€” Optimal Number of Clusters (DTW)")
plt.xlabel("Number of clusters (k)")
plt.ylabel("Total DTW Distance (Inertia)")
plt.grid(True)
plt.show()

"""**Exploratory Visualization of Time-Series Clusters**

This code visualizes the time-series sales patterns of retail businesses grouped by their DTW-based cluster assignments. First, it prints the number and names of business types in each cluster, offering interpretability into how categories are grouped. Then, for each cluster, it plots the standardized monthly sales (Z-scores) over time for all businesses in that cluster. These visualizations help uncover temporal similaritiesâ€”such as seasonality, trend, or volatilityâ€”shared within each group, enabling a clearer understanding of how different retail sectors respond to economic cycles, structural shifts, or external shocks like COVID-19.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.cm as cm
from tslearn.clustering import TimeSeriesKMeans

# Step 0: Calculate monthly log growth rate per business
df_merged = df_merged.copy()
df_merged['value'] = df_merged['value'].replace(0, np.nan)
df_merged['sales_growth'] = df_merged.groupby('kind_of_business')['value'].transform(
    lambda x: np.log(x).diff()
)

# Step 1: Drop rows with missing growth rates
df_growth_cleaned = df_merged.dropna(subset=['sales_growth'])

# Step 2: Pivot to shape (Date Ã— Business)
df_growth = df_growth_cleaned.groupby(['Date', 'kind_of_business'])['sales_growth'].mean().reset_index()
df_growth_matrix = df_growth.pivot(index='Date', columns='kind_of_business', values='sales_growth')

# Step 3: Transpose to (Businesses Ã— Time)
data_growth = df_growth_matrix.T.fillna(0).values

# Step 4: DTW-based clustering
model = TimeSeriesKMeans(n_clusters=4, metric="dtw", random_state=42)
labels = model.fit_predict(data_growth)

# Step 5: Assign labels
business_names = df_growth_matrix.columns.tolist()
df_cluster_labels = pd.DataFrame({
    'kind_of_business': business_names,
    'cluster': labels
})

# Step 6: Summary
cluster_counts = df_cluster_labels['cluster'].value_counts().sort_index()
print("ðŸ“Š Number of business types in each cluster:\n", cluster_counts)

# Step 7: Plot clusters (individual series + mean) in a 2Ã—2 grid
num_clusters = len(df_cluster_labels['cluster'].unique())
rows, cols = 2, 2  # for a 2x2 grid
fig, axs = plt.subplots(rows, cols, figsize=(16, 10), sharex=True)

axs = axs.flatten()  # Flatten to make indexing easy

for idx, c in enumerate(sorted(df_cluster_labels['cluster'].unique())):
    ax = axs[idx]
    businesses_in_cluster = df_cluster_labels[df_cluster_labels['cluster'] == c]['kind_of_business']

    print(f"\nðŸ“¦ Cluster {c}: {len(businesses_in_cluster)} businesses")
    print(sorted(businesses_in_cluster.tolist()))

    cluster_data = df_growth_matrix[businesses_in_cluster].fillna(0)
    cluster_array = cluster_data.values.T  # shape: (n_businesses, n_timepoints)

    # Use different color per series
    cmap = cm.get_cmap('tab20', cluster_array.shape[0])
    for i, series in enumerate(cluster_array):
        ax.plot(df_growth_matrix.index, series, color=cmap(i), alpha=0.7, linewidth=1)

    # Mean growth line
    cluster_mean = cluster_array.mean(axis=0)
    ax.plot(df_growth_matrix.index, cluster_mean, color='black', linewidth=2.2, alpha=0.85, label='Mean Growth')

    ax.set_title(f"Cluster {c} â€” Sales Growth Patterns")
    ax.set_ylabel("Log Growth")
    ax.grid(True)
    ax.legend()

for j in range(len(axs)):
    axs[j].label_outer()  # Hide x-labels and y-labels for inner plots

fig.suptitle("DTW Clustering of Business Sales Growth â€” 4 Clusters", fontsize=16)
fig.supxlabel("Date")
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt


# Ensure df_merged is available from previous steps and contains the 'cluster' column
if 'df_merged' not in globals():
    print("Error: Required dataframe (df_merged) is not available.")
elif 'cluster' not in df_merged.columns:
     print("Error: 'cluster' column not found in df_merged. Please ensure the clustering and subsequent merge steps were run correctly.")
else:
    # Use the existing df_merged which should already contain the 'cluster' column
    df_merged_with_clusters = df_merged.copy()

    # 2. Group by cluster and business, calculate business-level means
    # Select relevant numerical columns, excluding original 'value' and potentially others
    # based on which features you want in the correlation matrix
    features_for_summary = ['Inflation', 'gdp_growth_m', 'sales_growth', 'sales_lag1', 'sales_rolling_mean', 'regime']
    # Also include 'cluster' and 'kind_of_business' for grouping
    cols_to_include = ['cluster', 'kind_of_business'] + [f for f in features_for_summary if f in df_merged_with_clusters.columns]

    # Ensure all columns in cols_to_include are actually in df_merged_with_clusters
    cols_to_include = [col for col in cols_to_include if col in df_merged_with_clusters.columns]

    if not cols_to_include or len(cols_to_include) < 2: # Need at least cluster and kind_of_business
         print("Error: Not enough relevant columns found in df_merged for summary.")
    else:
        df_business_means = df_merged_with_clusters[cols_to_include].groupby(['cluster', 'kind_of_business']).mean().reset_index()

        # 3. Aggregate to cluster level (average of business means)
        # Also count the number of unique businesses per cluster
        agg_dict = {}
        for feature in features_for_summary:
            if feature in df_business_means.columns:
                 agg_dict[f'Avg_{feature}'] = (feature, 'mean')

        # Add Num_Series count
        agg_dict['Num_Series'] = ('kind_of_business', 'nunique')


        if not agg_dict:
             print("Error: No features available to aggregate for df_summary.")
        else:
            df_summary = df_business_means.groupby('cluster').agg(
                **agg_dict # Use the dynamically created aggregation dictionary
            ).reset_index()

            # Rename cluster column for consistency if needed (it should already be 'cluster')
            df_summary = df_summary.rename(columns={'cluster': 'Cluster'})

            # Display the created df_summary
            print("âœ… Created df_summary:")
            display(df_summary)

# Define your chosen features (make sure these columns exist in your DataFrame)
# Updated selected_features to match the column names in df_summary
selected_features = ['Cluster', 'Num_Series', 'Avg_sales_growth',
                     'Avg_Inflation', 'Avg_gdp_growth_m', 'Avg_sales_lag1', 'Avg_sales_rolling_mean', 'Avg_regime']


# Subset the DataFrame using the corrected selected_features
# Replace df_summary with your actual DataFrame name if it's different,
# but based on the previous cell, df_summary is the correct name.
df_corr = df_summary[selected_features].copy()

# Drop rows with missing values if needed
df_corr = df_corr.dropna()

# Compute the correlation matrix
# Exclude 'Cluster' from the correlation matrix computation as it's a label, not a feature for correlation
corr_matrix = df_corr.drop(columns=['Cluster']).corr()

# Plot the heatmap
plt.figure(figsize=(10, 8)) # Adjusted figure size for better readability
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f", vmin=-1, vmax=1,
            cbar_kws={'label': 'Pearson Correlation'})
plt.title('Correlation Heatmap of Cluster-Level Summary Statistics and Macro Indicators')
plt.tight_layout()

# Save the figure
plt.savefig("cluster_summary_correlation_heatmap.png", dpi=300)
plt.show()

"""**GDP-Based Regime Detection Using Markov Switching Model**

This code applies a two-regime Markov Switching model to smoothed monthly U.S. GDP growth to identify periods of economic expansion and recession. It plots the smoothed GDP growth alongside the estimated probability of being in a recession (Regime 1) over time.
"""

import statsmodels.api as sm
import matplotlib.pyplot as plt
import pandas as pd

# Step 0: Smooth GDP growth using a 3-month moving average
df_macro['gdp_growth_smoothed'] = df_macro['gdp_growth_m'].rolling(window=3, center=True).mean()
df_macro_smoothed = df_macro.dropna(subset=['gdp_growth_smoothed']).copy()

# Step 1: Fit Markov Switching model
mod_gdp = sm.tsa.MarkovAutoregression(df_macro_smoothed['gdp_growth_smoothed'],
                                       k_regimes=2, order=1, trend='c', switching_variance=True)
res_gdp = mod_gdp.fit()

# Step 2: Determine which regime is recession (lower mean)
mean_regime_0 = res_gdp.params['const[0]']
mean_regime_1 = res_gdp.params['const[1]']

if mean_regime_1 < mean_regime_0:
    df_macro_smoothed['prob_recession_gdp'] = res_gdp.smoothed_marginal_probabilities[1]
    recession_label = 'Regime 1 (Recession)'
    df_macro_smoothed['regime'] = (df_macro_smoothed['prob_recession_gdp'] > 0.5).astype(int)
else:
    df_macro_smoothed['prob_recession_gdp'] = res_gdp.smoothed_marginal_probabilities[0]
    recession_label = 'Regime 0 (Recession)'
    df_macro_smoothed['regime'] = (df_macro_smoothed['prob_recession_gdp'] > 0.5).astype(int)

# Ensure datetime
df_macro_smoothed['Date'] = pd.to_datetime(df_macro_smoothed['Date'])

# Step 3: Plot GDP growth + corrected regime probabilities
fig, axes = plt.subplots(2, 1, figsize=(12, 6), sharex=True)

# Top: GDP Growth
axes[0].plot(df_macro_smoothed['Date'], df_macro_smoothed['gdp_growth_smoothed'],
             label="Smoothed GDP Growth", color='seagreen')
axes[0].set_title("Smoothed Monthly U.S. GDP Growth Over Time")
axes[0].legend(loc='upper left')
axes[0].grid(True)

# Bottom: Smoothed probability of RECESSION (automatically identified)
axes[1].plot(df_macro_smoothed['Date'], df_macro_smoothed['prob_recession_gdp'],
             label=f"{recession_label} Probability", color='darkorange')
axes[1].set_title("Smoothed Recession Probability Based on GDP")
axes[1].set_ylim(0, 1)
axes[1].legend(loc='upper left')
axes[1].grid(True)

plt.tight_layout()
plt.show()

# Print regime summary
print(res_gdp.summary())

!pip install ruptures

"""**Structural Break Detection on OLS Residuals**
We apply structural break detection to the residuals of an Ordinary Least Squares (OLS) model estimated on the full 1992â€“2024 dataset. Using PELT and Binary Segmentation algorithms from the ruptures library, we identify breakpoints that reflect shifts in the relationship between macroeconomic indicators and retail sales growth. These detected changes often coincide with major economic disruptionsâ€”such as COVID-19 or inflationary shocksâ€”highlighting when the baseline model may no longer hold. This analysis serves as a diagnostic step to assess model stability and motivates the use of more flexible alternatives in later stages.


"""

import matplotlib.pyplot as plt
import ruptures as rpt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
import pandas as pd

# Optional: Reset to default matplotlib style (white background)
plt.style.use('default')

# Loop through clusters
for c in sorted(df_merged['cluster'].dropna().unique()):
    if c == 3:
        continue  # Skip Cluster 3 due to its volatility and low sample size

    # Filter and sort cluster data
    df_cluster = df_merged[df_merged['cluster'] == c].copy()
    df_cluster = df_cluster.sort_values('Date').reset_index(drop=True)

    if df_cluster.empty:
        continue

    # Drop rows with missing values in X or y
    df_cluster = df_cluster.dropna(subset=features + [target])

    # Extract features and target
    X = df_cluster[features].values
    y = df_cluster[target].values
    dates = pd.to_datetime(df_cluster['Date'])

    # Standardize features
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # Fit OLS model and compute residuals
    model = LinearRegression()
    model.fit(X_scaled, y)
    y_pred = model.predict(X_scaled)
    residuals = y - y_pred

    # Detect structural breaks using PELT
    signal = residuals.reshape(-1, 1)
    pelt_model = rpt.Pelt(model="l2").fit(signal)
    breakpoints = pelt_model.predict(pen=3)

    # === Plot results with white background and stronger lines ===
    fig, ax = plt.subplots(figsize=(12, 5), facecolor='white')  # White figure background
    ax.set_facecolor('white')  # White axis background

    ax.plot(dates, residuals, label='Residuals', color='mediumblue', linewidth=2.5)
    for i, bp in enumerate(breakpoints[:-1]):
        ax.axvline(
            dates.iloc[bp],
            color='red',
            linestyle='--',
            linewidth=2.2,
            alpha=1.0,
            label='Change Point' if i == 0 else ""
        )

    ax.set_title(f"Structural Breaks (PELT) â€” Residuals of OLS (Cluster {c})")
    ax.set_xlabel("Date")
    ax.set_ylabel("Residual")
    ax.grid(True, linestyle='--', alpha=0.6)
    ax.legend()
    plt.tight_layout()
    plt.savefig(f"residual_breaks_cluster_{c}.png", dpi=500, facecolor='white')
    plt.show()

"""**RidgeCV Diagnostics for Cluster-Based Retail Sales Growth Modeling**
This Python script performs a cluster-wise evaluation of Ridge regression models to analyze monthly log sales growth of various retail business categories prior to COVID-19. After preprocessing the dataâ€”including calculating log-differenced sales, lagged growth, and rolling meansâ€”the script filters the data for each cluster (excluding Cluster 3 due to its volatility). It standardizes the input features and fits a RidgeCV model, which uses cross-validation to automatically select the optimal regularization parameter (alpha). The model's performance is evaluated using Mean Squared Error (MSE) and R-squared (RÂ²) metrics, and diagnostic plots are generated: one for actual vs. predicted values, and another for residuals over time. These visualizations help assess model fit and identify potential misspecifications or structural patterns in the residuals. A final summary table presents the model diagnostics for each cluster. To maintain a clean output, the code suppresses any FutureWarning messages related to deprecated RidgeCV options.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import RidgeCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
import warnings

# === Suppress FutureWarnings (e.g., from RidgeCV) ===
warnings.filterwarnings("ignore", category=FutureWarning)

# === Settings ===
target = 'sales_growth'
covid_cutoff = pd.to_datetime("2020-03-01")
features = ['Inflation', 'gdp_growth_m', 'sales_lag1', 'sales_rolling_mean', 'regime']
alphas_to_try = np.logspace(-3, 3, 50)  # Alphas to test via cross-validation

results = []

# === Loop through clusters ===
for cluster_id in sorted(df_merged['cluster'].dropna().unique()):
    if cluster_id == 3:
        continue  # Skip cluster 3

    df_cluster = df_merged[df_merged['cluster'] == cluster_id].copy()
    df_cluster['Date'] = pd.to_datetime(df_cluster['Date'])
    df_cluster = df_cluster[df_cluster['Date'] < covid_cutoff].sort_values('Date')

    # Compute log sales growth
    df_cluster['value'] = df_cluster['value'].replace(0, np.nan)
    df_cluster['sales_growth'] = df_cluster.groupby('kind_of_business')['value'].transform(lambda x: np.log(x).diff())

    # Lagged features
    df_cluster['sales_lag1'] = df_cluster.groupby('kind_of_business')['sales_growth'].shift(1)
    df_cluster['sales_rolling_mean'] = df_cluster.groupby('kind_of_business')['sales_growth'].transform(
        lambda x: x.rolling(window=3).mean())

    df_cluster = df_cluster.dropna(subset=features + [target])

    if len(df_cluster) < 30:
        continue

    X = df_cluster[features].values
    y = df_cluster[target].values
    dates = df_cluster['Date']

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # Train RidgeCV (learn alpha automatically)
    model = RidgeCV(alphas=alphas_to_try, store_cv_values=True)
    model.fit(X_scaled, y)
    y_pred = model.predict(X_scaled)
    residuals = y - y_pred

    # Evaluation
    mse = mean_squared_error(y, y_pred)
    r2 = r2_score(y, y_pred)
    results.append({
        'Cluster': cluster_id,
        'Sample Size': len(df_cluster),
        'MSE': round(mse, 3),
        'RÂ²': round(r2, 3),
        'Best Alpha (CV)': round(model.alpha_, 4)
    })

    # === Plot: Left = Actual vs Predicted, Right = Residuals ===
    fig, axs = plt.subplots(1, 2, figsize=(14, 4), sharex=True)

    # Actual vs Predicted
    axs[0].plot(dates, y, label="Actual", linestyle='--', color='royalblue')
    axs[0].plot(dates, y_pred, label="Predicted", color='indianred')
    axs[0].set_title(f"Cluster {cluster_id} â€” Actual vs Predicted")
    axs[0].set_ylabel("Log Sales Growth")
    axs[0].set_xlabel("Date")
    axs[0].grid(True)
    axs[0].legend()

    # Residuals
    axs[1].plot(dates, residuals, color='teal')
    axs[1].axhline(0, linestyle='--', color='gray')
    axs[1].set_title(f"Cluster {cluster_id} â€” Residuals Over Time")
    axs[1].set_ylabel("Residual")
    axs[1].set_xlabel("Date")
    axs[1].grid(True)

    plt.suptitle(f"RidgeCV Diagnostics â€” Cluster {cluster_id}", fontsize=14)
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.show()

# === Final summary table ===
results_df = pd.DataFrame(results)
print("\nðŸ“‹ RidgeCV Results (Pre-COVID Only):")
print(results_df.to_string(index=False))

"""**Learning Curve Analysis for Ridge Regression by Cluster**
To evaluate the modelâ€™s generalization performance, we plotted learning curves for each of the three retail clusters by training Ridge regression models on progressively larger subsets of the pre-COVID training data and evaluating them on a fixed post-COVID test set. For Clusters 0 and 1, the target variable (sales_growth) was detrended prior to modeling to account for structural trends. The curves display Mean Squared Error (MSE) for both training and test sets across increasing training sizes, providing insight into the modelâ€™s learning dynamics. This analysis helps identify whether the model suffers from high bias or variance and whether increasing the training set size improves predictive accuracy across clusters.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import Ridge
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from scipy.signal import detrend

# === Constants ===
train_cutoff = pd.to_datetime("2020-03-01")
eval_start = pd.to_datetime("2015-03-01")  # March 2015
alpha_dict = {0: 2.68, 1: 1.53, 2: 4.71}
features = ['Inflation', 'gdp_growth_m', 'sales_lag1', 'sales_rolling_mean', 'regime']
target = 'sales_growth'
train_sizes = np.linspace(0.1, 1.0, 10)

plt.figure(figsize=(15, 10))

for i, cluster_id in enumerate([0, 1, 2]):
    df_cluster = df_merged[df_merged['cluster'] == cluster_id].copy()
    df_cluster['Date'] = pd.to_datetime(df_cluster['Date'])
    df_cluster = df_cluster.sort_values('Date')

    # Step 1: Compute sales growth if not already there
    if 'sales_growth' not in df_cluster.columns:
        df_cluster['value'] = df_cluster['value'].replace(0, np.nan)
        df_cluster['sales_growth'] = df_cluster.groupby('kind_of_business')['value'].transform(lambda x: np.log(x).diff())

    # Step 2: Feature Engineering
    df_cluster['sales_lag1'] = df_cluster.groupby('kind_of_business')['sales_growth'].shift(1)
    df_cluster['sales_rolling_mean'] = df_cluster.groupby('kind_of_business')['sales_growth'].transform(lambda x: x.rolling(3).mean())
    df_cluster = df_cluster.dropna(subset=features + [target])

    # Pre-COVID only
    df_pre_covid = df_cluster[df_cluster['Date'] < train_cutoff]

    # Split chronologically
    df_train = df_pre_covid[df_pre_covid['Date'] < eval_start]
    df_eval = df_pre_covid[(df_pre_covid['Date'] >= eval_start)]

    if len(df_train) < 80 or len(df_eval) < 30:
        continue

    X_train = df_train[features].values
    y_train_raw = df_train[target].values
    X_eval = df_eval[features].values
    y_eval_raw = df_eval[target].values

    # Detrend where needed
    y_train = detrend(y_train_raw) if cluster_id in [0, 1] else y_train_raw
    y_eval = detrend(y_eval_raw) if cluster_id in [0, 1] else y_eval_raw

    # Standardization
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_eval_scaled = scaler.transform(X_eval)

    alpha_val = alpha_dict.get(cluster_id, 1.0)

    train_errors = []
    eval_errors = []

    for frac in train_sizes:
        n_train = int(frac * len(X_train_scaled))
        if n_train < 10:
            continue

        X_part = X_train_scaled[:n_train]
        y_part = y_train[:n_train]

        model = Ridge(alpha=alpha_val)
        model.fit(X_part, y_part)

        train_pred = model.predict(X_part)
        eval_pred = model.predict(X_eval_scaled)

        train_mse = mean_squared_error(y_part, train_pred)
        eval_mse = mean_squared_error(y_eval, eval_pred)

        train_errors.append(train_mse)
        eval_errors.append(eval_mse)

    # Plotting
    plt.subplot(3, 1, i + 1)
    x_vals = train_sizes[:len(train_errors)] * len(X_train_scaled)
    plt.plot(x_vals, train_errors, label='Train MSE', marker='o')
    plt.plot(x_vals, eval_errors, label='Evaluation MSE', marker='s')
    plt.title(f"Learning Curve â€” Cluster {cluster_id}")
    plt.xlabel("Training Set Size")
    plt.ylabel("MSE")
    plt.grid(True)
    plt.legend()

plt.tight_layout()
plt.suptitle("Learning Curves (Train: 1992â€“2015 Feb, Eval: 2015 Marâ€“2020 Feb)", fontsize=16, y=1.02)
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import Ridge
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, f1_score, recall_score, precision_score
from scipy.signal import detrend

# === Constants ===
train_cutoff = pd.to_datetime("2020-03-01")
eval_start = pd.to_datetime("2015-03-01")
alpha_dict = {0: 2.68, 1: 1.53, 2: 4.71}
features = ['Inflation', 'gdp_growth_m', 'sales_lag1', 'sales_rolling_mean', 'regime']
target = 'sales_growth'
train_sizes = np.linspace(0.1, 1.0, 10)

fig, axes = plt.subplots(3, 4, figsize=(20, 12))  # 3 clusters Ã— 4 metrics

for i, cluster_id in enumerate([0, 1, 2]):
    df_cluster = df_merged[df_merged['cluster'] == cluster_id].copy()
    df_cluster['Date'] = pd.to_datetime(df_cluster['Date'])
    df_cluster = df_cluster.sort_values('Date')

    # Step 1: Compute sales growth
    df_cluster['value'] = df_cluster['value'].replace(0, np.nan)
    df_cluster['sales_growth'] = df_cluster.groupby('kind_of_business')['value'].transform(lambda x: np.log(x).diff())

    # Step 2: Feature engineering
    df_cluster['sales_lag1'] = df_cluster.groupby('kind_of_business')['sales_growth'].shift(1)
    df_cluster['sales_rolling_mean'] = df_cluster.groupby('kind_of_business')['sales_growth'].transform(lambda x: x.rolling(3).mean())
    df_cluster = df_cluster.dropna(subset=features + [target])

    # Filter pre-COVID period
    df_pre_covid = df_cluster[df_cluster['Date'] < train_cutoff]
    df_train = df_pre_covid[df_pre_covid['Date'] < eval_start]
    df_eval = df_pre_covid[df_pre_covid['Date'] >= eval_start]

    if len(df_train) < 80 or len(df_eval) < 30:
        continue

    X_train_full = df_train[features].values
    y_train_raw = df_train[target].values
    X_eval = df_eval[features].values
    y_eval_raw = df_eval[target].values

    # Apply detrending for Clusters 0 and 1
    y_train_full = detrend(y_train_raw) if cluster_id in [0, 1] else y_train_raw
    y_eval = detrend(y_eval_raw) if cluster_id in [0, 1] else y_eval_raw

    scaler = StandardScaler()
    X_train_full_scaled = scaler.fit_transform(X_train_full)
    X_eval_scaled = scaler.transform(X_eval)

    alpha_val = alpha_dict.get(cluster_id, 1.0)

    # === Store metrics
    train_errors, eval_errors = [], []
    train_f1s, eval_f1s = [], []
    train_precisions, eval_precisions = [], []
    train_recalls, eval_recalls = [], []

    for frac in train_sizes:
        n = int(frac * len(X_train_full_scaled))
        if n < 10:
            continue

        X_part = X_train_full_scaled[:n]
        y_part = y_train_full[:n]
        y_part_class = (y_part > 0).astype(int)
        y_eval_class = (y_eval > 0).astype(int)

        model = Ridge(alpha=alpha_val)
        model.fit(X_part, y_part)

        y_pred_train = model.predict(X_part)
        y_pred_eval = model.predict(X_eval_scaled)

        y_pred_train_class = (y_pred_train > 0).astype(int)
        y_pred_eval_class = (y_pred_eval > 0).astype(int)

        train_errors.append(mean_squared_error(y_part, y_pred_train))
        eval_errors.append(mean_squared_error(y_eval, y_pred_eval))
        train_f1s.append(f1_score(y_part_class, y_pred_train_class))
        eval_f1s.append(f1_score(y_eval_class, y_pred_eval_class))
        train_precisions.append(precision_score(y_part_class, y_pred_train_class))
        eval_precisions.append(precision_score(y_eval_class, y_pred_eval_class))
        train_recalls.append(recall_score(y_part_class, y_pred_train_class))
        eval_recalls.append(recall_score(y_eval_class, y_pred_eval_class))

    x_vals = train_sizes[:len(train_errors)] * len(X_train_full_scaled)

    # === Plotting
    axes[i, 0].plot(x_vals, train_errors, label='Train MSE', marker='o')
    axes[i, 0].plot(x_vals, eval_errors, label='Eval MSE', marker='s')
    axes[i, 0].set_title(f"Cluster {cluster_id} â€” MSE")
    axes[i, 0].legend()
    axes[i, 0].grid(True)

    axes[i, 1].plot(x_vals, train_f1s, label='Train F1', marker='o')
    axes[i, 1].plot(x_vals, eval_f1s, label='Eval F1', marker='s')
    axes[i, 1].set_title(f"Cluster {cluster_id} â€” F1 Score")
    axes[i, 1].legend()
    axes[i, 1].grid(True)

    axes[i, 2].plot(x_vals, train_precisions, label='Train Precision', marker='o')
    axes[i, 2].plot(x_vals, eval_precisions, label='Eval Precision', marker='s')
    axes[i, 2].set_title(f"Cluster {cluster_id} â€” Precision")
    axes[i, 2].legend()
    axes[i, 2].grid(True)

    axes[i, 3].plot(x_vals, train_recalls, label='Train Recall', marker='o')
    axes[i, 3].plot(x_vals, eval_recalls, label='Eval Recall', marker='s')
    axes[i, 3].set_title(f"Cluster {cluster_id} â€” Recall")
    axes[i, 3].legend()
    axes[i, 3].grid(True)

# === Final plot formatting
plt.tight_layout()
plt.suptitle("Learning Curves (Train: 1992â€“Feb 2015, Eval: Mar 2015â€“Feb 2020, Ridge Regression)", fontsize=16, y=1.02)
plt.show()

"""**Ridge Regression Analysis of Retail Sales Growth Across Business Clusters**
This analysis applies Ridge regression to model and evaluate the monthly log sales growth of retail businesses across three primary clusters (Clusters 0, 1, and 2). Each cluster, previously identified through time-series clustering, represents a distinct pattern of sales behavior. The model incorporates both macroeconomic indicators (inflation, GDP growth, and economic regime) and sales dynamics (lagged growth and rolling averages) as explanatory variables. The data is divided into a training period (pre-COVID) and a testing period (post-2022) to assess out-of-sample performance. For Clusters 0 and 1, a detrending step is applied to focus on short-term fluctuations. Ridge regression is fit using cluster-specific regularization strengths, and model performance is evaluated using Mean Squared Error (MSE) and R-squared (RÂ²) for both training and testing phases. Visualization panels are created for each cluster, showing actual vs. predicted growth and residual patterns over time. These residuals help identify potential structural breaks or changes in sales dynamics. A summary table concludes the analysis, offering a concise view of model accuracy across clusters.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import Ridge
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
from scipy.signal import detrend

# Settings
target = 'sales_growth'
train_cutoff = pd.to_datetime("2020-03-01")
test_start = pd.to_datetime("2022-01-01")
alpha_dict = {0: 2.68, 1: 1.53, 2: 4.71}
features = ['Inflation', 'gdp_growth_m', 'sales_lag1', 'sales_rolling_mean', 'regime']

results = []
fig, axes = plt.subplots(3, 2, figsize=(14, 12))  # 3 rows (clusters), 2 columns (prediction + residual)

# Loop through the specific clusters you want to plot
# Ensure the loop iterates only over the clusters for which axes were created
for row_idx, cluster_id in enumerate([0, 1, 2]): # Explicitly loop through clusters 0, 1, 2
    df_cluster = df_merged[df_merged['cluster'] == cluster_id].copy()
    df_cluster['Date'] = pd.to_datetime(df_cluster['Date'])
    df_cluster = df_cluster.sort_values('Date')

    # Feature Engineering
    df_cluster['sales_lag1'] = df_cluster.groupby('kind_of_business')['sales_growth'].shift(1)
    df_cluster['sales_rolling_mean'] = df_cluster.groupby('kind_of_business')['sales_growth'].transform(lambda x: x.rolling(3).mean())
    df_cluster = df_cluster.dropna(subset=features + [target])

    df_train = df_cluster[df_cluster['Date'] < train_cutoff]
    df_test = df_cluster[df_cluster['Date'] >= test_start]

    if len(df_train) < 50 or len(df_test) < 30:
        print(f"âš ï¸ Skipping Cluster {cluster_id}: not enough data.")
        continue

    X_train = df_train[features].values
    y_train_raw = df_train[target].values
    X_test = df_test[features].values
    y_test_raw = df_test[target].values
    dates_train = df_train['Date'].values
    dates_test = df_test['Date'].values


    # Detrend clusters 0 & 1
    y_train = detrend(y_train_raw) if cluster_id in [0, 1] else y_train_raw
    y_test = detrend(y_test_raw) if cluster_id in [0, 1] else y_test_raw

    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    alpha_val = alpha_dict.get(cluster_id, 1.0)
    model = Ridge(alpha=alpha_val)
    model.fit(X_train_scaled, y_train)
    y_pred_train = model.predict(X_train_scaled)
    y_pred_test = model.predict(X_test_scaled)

    residuals_train = y_train - y_pred_train
    residuals_test = y_test - y_pred_test

    # Combine for plotting
    dates_combined = np.concatenate([dates_train, dates_test])
    actual_combined = np.concatenate([y_train_raw, y_test_raw])
    predicted_combined = np.concatenate([y_pred_train, y_pred_test])
    residuals_combined = np.concatenate([residuals_train, residuals_test])


    # Store results
    results.append({
        'Cluster': cluster_id,
        'Train Size': len(df_train),
        'Test Size': len(df_test),
        'Train MSE': round(mean_squared_error(y_train, y_pred_train), 3),
        'Train RÂ²': round(r2_score(y_train, y_pred_train), 3),
        'Test MSE': round(mean_squared_error(y_test, y_pred_test), 3),
        'Test RÂ²': round(r2_score(y_test, y_pred_test), 3)
    })

    # === Plot 1: Actual vs Predicted
    ax1 = axes[row_idx, 0]
    ax1.plot(dates_combined, actual_combined, label="Actual", linestyle='--', color='maroon')
    ax1.plot(dates_combined, predicted_combined, label="Predicted", color='skyblue')
    ax1.axvline(train_cutoff, linestyle=':', color='gray', label='COVID Start')
    ax1.axvline(test_start, linestyle=':', color='black', label='Test Start')
    ax1.set_title(f"Cluster {cluster_id} â€” Predicted vs Actual")
    ax1.set_ylabel("Log Sales Growth")
    ax1.grid(True)
    if row_idx == 0: # Add legend only to the first plot for clarity
        ax1.legend()


    # === Plot 2: Residuals
    ax2 = axes[row_idx, 1]
    ax2.plot(dates_combined, residuals_combined, label='Residuals', color='teal')
    ax2.axhline(0, linestyle='--', color='gray')
    ax2.set_title(f"Cluster {cluster_id} â€” Residuals")
    ax2.set_ylabel("Residual")
    ax2.grid(True)


# === Final Touch
plt.tight_layout()
plt.suptitle("Ridge Regression: Actual vs Predicted and Residuals by Cluster", fontsize=16, y=1.02)
plt.show()


# Summary Table
results_df = pd.DataFrame(results)
print("\nðŸ“‹ Ridge Regression Results (Simplified Model):")
print(results_df.to_string(index=False))

"""**Cross-Validated Ridge Regression Model (Out-of-Sample, Pre/Post-COVID)**
This model evaluates out-of-sample performance of Ridge regression by training on pre-COVID retail sales data (before March 2020) and testing on a post-COVID period beginning in January 2022, excluding the volatile pandemic period in between. Separate models are estimated for each cluster of retail businesses using a consistent set of standardized predictors, including inflation, monthly GDP growth, a regime indicator (from a Markov Switching model), and lagged sales metrics (sales_lag1, sales_rolling_mean).

Ridge regression is applied with alpha selected via 5-fold cross-validation from a log-spaced range of values. For each cluster, we report the cross-validated alpha, mean squared error (MSE), and R-squared (RÂ²) on both training and test sets. We also extract the estimated coefficients to examine the relative importance of features across clusters. This approach helps identify how macroeconomic shocks and past sales dynamics influence future sales growth in different retail sectors under varying macro regimes.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import RidgeCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score

# Time boundaries
train_cutoff = pd.to_datetime("2020-03-01")
test_start = pd.to_datetime("2022-01-01")
target = 'sales_growth'

results = []
all_coefs = []

# Define the alpha values to try in RidgeCV
alphas_to_try = np.logspace(-3, 2, 30)

# Define features
features = ['Inflation', 'gdp_growth_m', 'sales_lag1', 'sales_rolling_mean', 'regime']

# Loop over clusters
for cluster_id in sorted(df_merged['cluster'].dropna().unique()):
    df_cluster = df_merged[df_merged['cluster'] == cluster_id].copy()
    df_cluster['Date'] = pd.to_datetime(df_cluster['Date'])
    df_cluster = df_cluster.sort_values('Date')

    # Feature Engineering
    df_cluster['sales_lag1'] = df_cluster.groupby('kind_of_business')['sales_growth'].shift(1)
    df_cluster['sales_rolling_mean'] = df_cluster.groupby('kind_of_business')['sales_growth'].transform(
        lambda x: x.rolling(3).mean()
    )

    df_cluster = df_cluster.dropna(subset=features + [target])

    # Train-test split
    df_train = df_cluster[df_cluster['Date'] < train_cutoff]
    df_test = df_cluster[df_cluster['Date'] >= test_start]

    if len(df_train) < 50 or len(df_test) < 30:
        print(f"âš ï¸ Skipping Cluster {cluster_id}: not enough data.")
        continue

    # Prepare inputs
    X_train = df_train[features].values
    y_train = df_train[target].values
    X_test = df_test[features].values
    y_test = df_test[target].values

    # Standardize features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Ridge regression with cross-validation
    model = RidgeCV(alphas=alphas_to_try, store_cv_results=True)
    model.fit(X_train_scaled, y_train)

    # Predictions
    y_pred_train = model.predict(X_train_scaled)
    y_pred_test = model.predict(X_test_scaled)

    # Residuals
    residuals_train = y_train - y_pred_train
    residuals_test = y_test - y_pred_test

    # Performance metrics
    mse_train = mean_squared_error(y_train, y_pred_train)
    r2_train = r2_score(y_train, y_pred_train)
    mse_test = mean_squared_error(y_test, y_pred_test)
    r2_test = r2_score(y_test, y_pred_test)

    results.append({
        'Cluster': cluster_id,
        'Train Size': len(df_train),
        'Test Size': len(df_test),
        'Alpha (CV)': round(model.alpha_, 4),
        'Train MSE': round(mse_train, 3),
        'Train RÂ²': round(r2_train, 3),
        'Test MSE': round(mse_test, 3),
        'Test RÂ²': round(r2_test, 3)
    })

    # Store coefficients
    for f, coef in zip(features, model.coef_):
        all_coefs.append({
            'Cluster': cluster_id,
            'Feature': f,
            'Coefficient': round(coef, 4)
        })

# === Output Results ===
results_df = pd.DataFrame(results)
coef_df = pd.DataFrame(all_coefs)

print("\nðŸ“‹ Ridge Regression Performance (with CV-selected alpha):")
print(results_df.to_string(index=False))

print("\nðŸ“ˆ Ridge Coefficients by Cluster:")
print(coef_df.to_string(index=False))

!pip install ruptures